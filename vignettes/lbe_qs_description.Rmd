---
title: "QA Based On LBE GE Guide"
author: "Peter von Rohr"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{QA Report for LBE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## QA Requirements Taken From the GE-Recipe
The following list of points is taken from https://qualitasag.atlassian.net/wiki/spaces/ZWS/pages/250314857/Anleitung+ZWS+LBE+bv+je+und+rh#AnleitungZWSLBEbv,jeundrh-LBE_Qualitaetssicherung and it describes the logfiles that need to be taken into account for QA. 

1. Nach ca. 1 Stunde nach dem Start von EVAL_DIR/prog/chef.sh verschiedene Outputs und Log-Dateien prüfen und mit den entsprechenden Dateien aus der vorhergehenden Auswertung vergleichen. Diese Dateien sind in `EVAL_DIR/data/${Zuchtorganisation}` und heissen:
    + mehrfacheLBEs.txt: Duplikate der ZO melden
    + checkSchema.log: Hier ist insbesondere ist darauf zu achten, dass es bei allen Schema, bei denen es Datenzuwachs geben sollte, diesen auch tatsächlich im üblichen Masse gibt.
    + abgelehnteLBEsWegenCheckSchema.csv: Falls es eine grosse Zunahme an Einträgen in dieser Datei gibt, weist das auf ein neues Schema oder auf Fehler/Ungenauigkeiten in den Dateien mit Schemaspezifikationen hin.
    + selectAnimalsOfBreed_*.log
    + wurstleDatenZurecht_*_*.log
    + plausiLBE_*.log
    + fehlerhafteLBEs_*.txt
    + macheBeobPositiv.log
2. Nur bei Basisanpassung: Kontrollieren ob die Basistiere richtig eingestellt (vor allem bezüglich Geburtsjahrgänge) sind.
3. Vergleichsgrafiken und -Tabellen analysieren.
4. In der Datei /qualstorzws01/data_zws/zwKontrolle/ZuwachsDatenZWS_BV_JE_RH.xls die Anzahl Records der Input- und Output-Dateien eintragen sowie Differenzen zur vorhergehenden ZWS rechnen.


## Tool Development
### Number of duplicate LBE records
The first thing that we have to check are the number of duplicates. The number of duplicate information corresponds to the number of lines in the file `mehrfacheLBEs.txt`. The number of lines in the file can be determined by using the `wc -l` command on the shell. For a given duplicate logfile the number of lines can be determined by

```{r}
s_dup_lf <- system.file("extdata/lbe-logs/mehrfacheLBEs.txt", package = 'zwsroutinetools')
s_nr_dupl <- system(paste0('wc -l ', s_dup_lf), intern = TRUE)
s_nr_dupl<- gsub(pattern = '^\\s{1,}', replacement = '', s_nr_dupl)
n_nr_dupl <- as.numeric(unlist(strsplit(s_nr_dupl, split = '\\s{1,}'))[1])
cat("The number of duplicate LBEs is: ", n_nr_dupl, "\n")
```

This is converted into the following function

```{r}
get_nr_line_file <- function(ps_file_path){
  # check whether the file exists
  if (!file.exists(ps_file_path)) stop("ERROR: Cannot find file: ", ps_file_path)
  # do the line counting with wc -l
  s_nr_dupl <- system(paste('wc -l', ps_file_path), intern = TRUE)
  # replace leading spaces from result
  s_nr_dupl<- gsub(pattern = '^\\s{1,}', replacement = '', s_nr_dupl)
  # return the number of lines corresponding to the first element of the result as a number
  return(as.numeric(unlist(strsplit(s_nr_dupl, split = '\\s{1,}'))[1]))
}
```

The test with the above used file is done as

```{r}
cat("The number of duplicate LBEs is: ", get_nr_line_file(ps_file_path = s_dup_lf), "\n")
```


### Data Increments over Schemas
The logfile `checkSchema.log` contains information about the distribution of the data records accross the different recording schemas. To extract the information on the number of records per recording scheme, we have to read the logfile and parse it to find the relevant information.

```{r}
s_check_log_path <- system.file("extdata/lbe-logs/checkSchema.log", package = 'zwsroutinetools')
con_log <- file(description = s_check_log_path)
vec_log <- readLines(con = con_log)
close(con = con_log)
# remove empty lines from log file
vec_idx_empty_lines <- grep(pattern = '^$', vec_log)
vec_log <- vec_log[-vec_idx_empty_lines]
# search for lines containing information about number of records per schema
vec_idx_schema_info <- grep(pattern = '^\\s{1,}Anzahl LBEs aus', vec_log)
vec_schema_info <- vec_log[vec_idx_schema_info]
get_schema_stat <- function(ps_line){
  vec_line_cont <- unlist(strsplit(ps_line, split = '\\s{1,}'))
  n_schema_name_idx <- grep(pattern = "Schema", vec_line_cont, fixed = TRUE) + 1
  return(tibble::tibble(schema = vec_line_cont[n_schema_name_idx], 
                        nrecords = vec_line_cont[length(vec_line_cont)]))
}
```

The above extraction function can be tested for a single line as 

```{r}
get_schema_stat(vec_schema_info[1])
```

As the result seams to be ok, we are applying the function to all schema records using the `purrr::map` function. Finally, we are re-grouping the result into a single `tibble`. 

```{r}
l_result <- purrr::map(vec_schema_info, get_schema_stat)
tbl_result <- NULL
for (i in seq_along(l_result)){
  tbl_result <- dplyr::bind_rows(tbl_result, l_result[[i]])
}
```

Re-formating the result tibble into a table leads to 

```{r}
knitr::kable(tbl_result, booktabs = TRUE)
```

The complete functionality of extracting the number of records from a logfile is enclosed in a separate function. 

```{r}
#' Get Schema Statistics from Logfile
get_schema_stat_from_log <- function(ps_check_log_path, 
                                     ps_schema_line_pat = '^\\s{1,}Anzahl LBEs aus',
                                     ps_schema_num_pat  = 'Schema'){
  #' Helper function to extract tibble with schema name and number of 
  #'  records for corresponding schema
  get_schema_stat <- function(ps_line, ps_schema_num_pat){
   vec_line_cont <- unlist(strsplit(ps_line, split = '\\s{1,}'))
    n_schema_name_idx <- grep(pattern = ps_schema_num_pat, vec_line_cont, fixed = TRUE) + 1
    return(tibble::tibble(schema = vec_line_cont[n_schema_name_idx], 
                          nrecords = vec_line_cont[length(vec_line_cont)]))
  }
  # Read the log file into a vector of characters
  con_log <- file(description = ps_check_log_path)
  vec_log <- readLines(con = con_log)
  close(con = con_log)
  # remove empty lines from log file
  vec_idx_empty_lines <- grep(pattern = '^$', vec_log)
  vec_log <- vec_log[-vec_idx_empty_lines]
  # search for lines containing information about number of records per schema
  vec_idx_schema_info <- grep(pattern = ps_schema_line_pat, vec_log)
  vec_schema_info <- vec_log[vec_idx_schema_info]
  # run the extractor function get_schema_stat for each line of vec_schema_info
  l_result <- purrr::map(vec_schema_info, get_schema_stat, ps_schema_num_pat)
  # aggregate list of results into single tibble
  tbl_result <- NULL
  for (i in seq_along(l_result)){
    tbl_result <- dplyr::bind_rows(tbl_result, l_result[[i]])
  }
  # return result
  return(tbl_result)
}
```

Testing the created function leads to 

```{r}
tbl_schema_rec_result <- get_schema_stat_from_log(ps_check_log_path = s_check_log_path)
knitr::kable(tbl_schema_rec_result, booktabs = TRUE)
```

